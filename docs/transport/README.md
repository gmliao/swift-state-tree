[English](README.md) | [中文版](README.zh-TW.md)

# Transport

Transport layer connects network connections with LandKeeper, with the core goal of "not exposing network details to Land DSL".

## Design Focus

- Land layer only cares about `LandContext` and handlers, doesn't know WebSocket/HTTP details
- Transport layer is responsible for connection management, message serialization, and routing
- Synchronization uses snapshot + diff to avoid blocking state mutation

## Data Flow

```
Client ↔ WebSocketTransport ↔ TransportAdapter ↔ LandKeeper
```

## Three-Layer Identification

- `playerID`: Account/player identity (business layer)
- `clientID`: Device or client instance (provided by application)
- `sessionID`: Connection layer identification (generated by server)

These three layers of identification enter `LandContext`, giving handlers a consistent interface,
and supporting multi-device, multi-connection behavior control.

## Core Components

- Transport protocol: Abstract connection behavior (start/stop/send)
- WebSocketTransport: Default WebSocket implementation
- TransportAdapter: Parse messages and call LandKeeper
- LandManager / Registry: Manage multiple rooms
- LandRouter / LandRealm: Cross-room routing and control

## Join and Session Flow

- Client must explicitly send join request
- Join creates `PlayerSession` and executes `CanJoin` / `OnJoin` in order
- Initial join returns snapshot, subsequent updates use `StateUpdate` (firstSync/diff)

PlayerSession field priority:

1. join request content
2. JWT payload
3. guest session

## Synchronization Model

- snapshot: Complete state (including per-player filtering)
- diff: Only send changes (path-based patches)
- firstSync: Sent once after player's cache is first established

## Error Handling

- join/action/event errors return `ErrorPayload`
- Join checks landID and session state to avoid duplicate joins or incorrect routing

## Multi-Room Support

- `LandManager` manages Land lifecycle and queries
- `LandRouter` routes connections to corresponding land
- In multi-room mode, join routes based on landType + instanceId

## Parallel Encoding

> **Status Note**: Parallel encoding feature has been implemented and tested, but current effects are unclear. In synthetic test environments, benefits mostly fall in the 1.0–1.2x range, with limited improvement. **Currently disabled by default**. Future real bot testing (including complete action/tick/transport/IO) is needed to more clearly define actual effects. To enable, explicitly pass `enableParallelEncoding: true` when creating `TransportAdapter`.

`TransportAdapter` supports parallel encoding of state updates to improve performance in multi-player scenarios.

### Basic Mode

> **Note**: Parallel encoding is currently disabled by default. To enable, explicitly pass `enableParallelEncoding: true` when creating `TransportAdapter`.

When parallel encoding is enabled, state updates for multiple players use `TaskGroup` for parallel encoding:

```swift
let adapter = TransportAdapter(
    keeper: keeper,
    transport: transport,
    landID: landID,
    enableParallelEncoding: true
)
```

### Parallel Encoding Behavior

Parallel encoding automatically adjusts concurrency based on player count and room configuration:

**Enable Conditions**:
- Player count >= `minPlayerCount` (default 20)
- Using JSON codec (thread-safe)

**Concurrency Calculation**:
```swift
concurrency = min(perRoomCap, batchWorkers, pendingUpdateCount)
```

Where:
- **`perRoomCap`**: Per-room concurrency cap
  - Player count < 30: `lowCap = 2`
  - Player count >= 30: `highCap = 4`
- **`batchWorkers`**: Calculated based on batch size
  - `batchWorkers = ceil(playerCount / batchSize)`
  - `batchSize = 12` (default)
- **`pendingUpdateCount`**: Number of pending updates

**Note**: Concurrency is determined by `perRoomCap` and `batchWorkers`, usually much smaller than CPU core count, so actual concurrency won't exceed system capacity. Swift's `TaskGroup` automatically manages task queuing, even if many tasks are created, the system will only run approximately equal to CPU core count tasks simultaneously.

**Examples**:
- 30 players: `perRoomCap=4`, `batchWorkers=3` → `concurrency=3` (limited by `batchWorkers`)
- 50 players: `perRoomCap=4`, `batchWorkers=5` → `concurrency=4` (limited by `perRoomCap`)

### Multi-Room Scenarios

In multi-room scenarios, each `TransportAdapter` instance only manages one room and cannot automatically know how many rooms exist in the system. Each room's concurrency is determined by `perRoomCap` and `batchWorkers`, usually much smaller than CPU core count. Swift's `TaskGroup` automatically manages task queuing, even if multiple rooms create many tasks simultaneously, the system will only run approximately equal to CPU core count tasks simultaneously.

### Configuration Parameters

You can adjust parallel encoding parameters through the following methods:

```swift
// Set minimum player count threshold
await adapter.setParallelEncodingMinPlayerCount(20)

// Set batch size
await adapter.setParallelEncodingBatchSize(12)

// Set concurrency caps
await adapter.setParallelEncodingConcurrencyCaps(
    lowPlayerCap: 2,      // Small room cap
    highPlayerCap: 4,     // Large room cap
    highPlayerThreshold: 30  // Switch threshold
)
```

### Performance Considerations

- **Small rooms (< 20 players)**: Parallel encoding won't be enabled (below threshold)
- **Medium rooms (20-30 players)**: Concurrency approximately 2-3
- **Large rooms (30+ players)**: Concurrency approximately 3-4 (limited by `perRoomCap`)

Swift's `TaskGroup` automatically manages task queuing and scheduling, even if many tasks are created, the system will only run approximately equal to CPU core count tasks simultaneously.
